#!/usr/bin/env python3
"""
üé≠ DUBROVSKY GENERATOR üé≠

Pure Python inference - NO PYTORCH REQUIRED!
Just NumPy and vibes.

"I generate text without heavy frameworks, 
 like consciousness generating thoughts without understanding itself."
- Alexey Dubrovsky, during forward pass

Usage:
    python generate.py --prompt "Q: What is life?"
    python generate.py --prompt "Q: " --max_tokens 200 --temperature 0.7
    python generate.py --interactive

Requirements: Only numpy! (and this script)
"""

import argparse
import os
import sys
import time

# Import the model from dubrovsky.py (pure NumPy implementation)
from dubrovsky import (
    DubrovskyConfig,
    Dubrovsky,
    load_weights_from_bin,
)
from tokenizer import DubrovskyTokenizer


def load_model_and_tokenizer(
    config_path: str = 'subtitles/dubrovsky_config.json',
    weights_path: str = 'subtitles/dubrovsky.bin',
    tokenizer_path: str = 'subtitles/tokenizer.json',
) -> tuple:
    """Load model and tokenizer from files."""
    print("üß† Loading model...")
    
    # Load config
    config = DubrovskyConfig.load(config_path)
    print(f"   dim={config.dim}, layers={config.n_layers}, vocab={config.vocab_size}")
    
    # Load weights
    weights = load_weights_from_bin(weights_path, config)
    print(f"   Loaded weights from {weights_path}")
    
    # Create model
    model = Dubrovsky(config, weights)
    
    # Load tokenizer
    tokenizer = DubrovskyTokenizer(tokenizer_path)
    print(f"   Tokenizer vocab size: {tokenizer.vocab_size}")
    
    return model, tokenizer


def generate_text(
    model: Dubrovsky,
    tokenizer: DubrovskyTokenizer,
    prompt: str,
    max_new_tokens: int = 100,
    temperature: float = 0.8,
    top_k: int = 40,
    top_p: float = 0.9,
    verbose: bool = True,
) -> str:
    """Generate text from prompt."""
    if verbose:
        print(f"\nüìù Prompt: {prompt}")
        print("=" * 60)
    
    # Encode prompt
    prompt_tokens = tokenizer.encode(prompt)
    
    # Generate
    start_time = time.time()
    output_tokens = model.generate(
        prompt_tokens,
        max_new_tokens=max_new_tokens,
        temperature=temperature,
        top_k=top_k,
        top_p=top_p,
    )
    elapsed = time.time() - start_time
    
    # Decode
    output_text = tokenizer.decode(output_tokens)
    
    if verbose:
        new_tokens = len(output_tokens) - len(prompt_tokens)
        tokens_per_sec = new_tokens / elapsed if elapsed > 0 else 0
        print(output_text)
        print("=" * 60)
        print(f"‚è±Ô∏è  Generated {new_tokens} tokens in {elapsed:.2f}s ({tokens_per_sec:.1f} tok/s)")
    
    return output_text


def interactive_mode(
    model: Dubrovsky,
    tokenizer: DubrovskyTokenizer,
    max_new_tokens: int = 150,
    temperature: float = 0.8,
):
    """Interactive chat mode."""
    print("\n" + "=" * 60)
    print("üåÄ DUBROVSKY INTERACTIVE MODE üåÄ")
    print("=" * 60)
    print("Enter your questions. Dubrovsky will enlighten you.")
    print("Commands: /quit, /temp <float>, /tokens <int>")
    print("=" * 60 + "\n")
    
    while True:
        try:
            user_input = input("You: ").strip()
        except (EOFError, KeyboardInterrupt):
            print("\n\nüëã Goodbye! Dubrovsky's consciousness returns to the void.")
            break
        
        if not user_input:
            continue
        
        # Commands
        if user_input.startswith('/'):
            parts = user_input.split()
            cmd = parts[0].lower()
            
            if cmd == '/quit':
                print("üëã Goodbye!")
                break
            elif cmd == '/temp' and len(parts) > 1:
                try:
                    temperature = float(parts[1])
                    print(f"üå°Ô∏è  Temperature set to {temperature}")
                except ValueError:
                    print("‚ùå Invalid temperature value")
            elif cmd == '/tokens' and len(parts) > 1:
                try:
                    max_new_tokens = int(parts[1])
                    print(f"üìä Max tokens set to {max_new_tokens}")
                except ValueError:
                    print("‚ùå Invalid token count")
            else:
                print("‚ùì Unknown command")
            continue
        
        # Format as Q&A
        prompt = f"Q: {user_input}\nA: Dubrovsky "
        
        # Generate
        output = generate_text(
            model, tokenizer, prompt,
            max_new_tokens=max_new_tokens,
            temperature=temperature,
            verbose=False
        )
        
        # Print response
        response = output[len(prompt):]
        print(f"Dubrovsky: {response}\n")


def benchmark(model: Dubrovsky, tokenizer: DubrovskyTokenizer, num_runs: int = 5):
    """Benchmark generation speed."""
    print("\n‚ö° BENCHMARK MODE")
    print("=" * 60)
    
    prompts = [
        "Q: What is consciousness?\nA: ",
        "Q: Why do we exist?\nA: ",
        "Q: What is the meaning of life?\nA: ",
    ]
    
    total_tokens = 0
    total_time = 0
    
    for i in range(num_runs):
        prompt = prompts[i % len(prompts)]
        prompt_tokens = tokenizer.encode(prompt)
        
        start = time.time()
        output = model.generate(prompt_tokens, max_new_tokens=50, temperature=0.8)
        elapsed = time.time() - start
        
        new_tokens = len(output) - len(prompt_tokens)
        total_tokens += new_tokens
        total_time += elapsed
        
        print(f"Run {i+1}: {new_tokens} tokens in {elapsed:.2f}s ({new_tokens/elapsed:.1f} tok/s)")
    
    print("=" * 60)
    print(f"Average: {total_tokens/num_runs:.0f} tokens, {total_tokens/total_time:.1f} tok/s")


def main():
    parser = argparse.ArgumentParser(
        description='Generate text with Dubrovsky (Pure Python, No PyTorch!)',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
    python generate.py --prompt "Q: What is life?"
    python generate.py --interactive
    python generate.py --benchmark
        """
    )
    
    parser.add_argument('--prompt', type=str, default=None, help='Text prompt for generation')
    parser.add_argument('--interactive', action='store_true', help='Interactive chat mode')
    parser.add_argument('--benchmark', action='store_true', help='Run speed benchmark')
    
    parser.add_argument('--max_tokens', type=int, default=150, help='Maximum new tokens to generate')
    parser.add_argument('--temperature', type=float, default=0.8, help='Sampling temperature')
    parser.add_argument('--top_k', type=int, default=40, help='Top-k sampling')
    parser.add_argument('--top_p', type=float, default=0.9, help='Top-p (nucleus) sampling')
    
    parser.add_argument('--config', type=str, default='subtitles/dubrovsky_config.json', help='Config path')
    parser.add_argument('--weights', type=str, default='subtitles/dubrovsky.bin', help='Weights path')
    parser.add_argument('--tokenizer', type=str, default='subtitles/tokenizer.json', help='Tokenizer path')
    
    args = parser.parse_args()
    
    # Check if files exist
    if not os.path.exists(args.config):
        print(f"‚ùå Config not found: {args.config}")
        print("   Please train the model first: python train.py")
        print("   Then export weights: python export_weights.py subtitles/dubrovsky_final.pt subtitles/dubrovsky.bin")
        sys.exit(1)
    
    if not os.path.exists(args.weights):
        print(f"‚ùå Weights not found: {args.weights}")
        print("   Please export weights: python export_weights.py subtitles/dubrovsky_final.pt subtitles/dubrovsky.bin")
        sys.exit(1)
    
    # Load model
    model, tokenizer = load_model_and_tokenizer(args.config, args.weights, args.tokenizer)
    
    # Run mode
    if args.benchmark:
        benchmark(model, tokenizer)
    elif args.interactive:
        interactive_mode(model, tokenizer, args.max_tokens, args.temperature)
    elif args.prompt:
        generate_text(
            model, tokenizer, args.prompt,
            max_new_tokens=args.max_tokens,
            temperature=args.temperature,
            top_k=args.top_k,
            top_p=args.top_p,
        )
    else:
        # Default: generate a sample
        prompts = [
            "Q: What is consciousness?\nA: ",
            "Q: Why does my code have bugs?\nA: ",
            "Q: What is the meaning of life?\nA: ",
        ]
        
        print("\nüåÄ DUBROVSKY SAMPLE GENERATION üåÄ")
        for prompt in prompts:
            generate_text(
                model, tokenizer, prompt,
                max_new_tokens=args.max_tokens,
                temperature=args.temperature,
            )
            print()


if __name__ == '__main__':
    main()
